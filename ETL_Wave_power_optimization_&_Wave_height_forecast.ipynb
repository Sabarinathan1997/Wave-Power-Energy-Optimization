{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7509db8c-4384-4b97-8084-8131e65478fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the content to my_etl_pipeline.py\n",
    "etl_pipeline_content = \"\"\"\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from dagster import job, op\n",
    "from sqlalchemy import create_engine\n",
    "import numpy as np\n",
    "\n",
    "# Paths to the datasets\n",
    "file_path_1 = \"Galway bay test site.csv\"\n",
    "file_path_2 = \"Wave buoy_Sligo.csv\"\n",
    "\n",
    "# Define the connection string\n",
    "connection_string = \"postgresql://dap:dap@127.0.0.1:5432/Wave energy output\"\n",
    "batch_size = 1000\n",
    "\n",
    "@op\n",
    "def galway_bay_test_site_extract_data(context):\n",
    "    test_site_data = pd.read_csv(file_path_1, low_memory=False)\n",
    "    context.log.info(f\"Extracted {len(test_site_data)} rows from {file_path_1}\")\n",
    "    return test_site_data\n",
    "\n",
    "@op\n",
    "def wave_buoy_sligo_extract_data(context):\n",
    "    buoy_data = pd.read_csv(file_path_2, low_memory=False)\n",
    "    context.log.info(f\"Extracted {len(buoy_data)} rows from {file_path_2}\")\n",
    "    return buoy_data\n",
    "\n",
    "@op\n",
    "def galway_bay_test_site_clean_data(context, data):\n",
    "    data_cleaned = data.apply(pd.to_numeric, errors='coerce')  # Convert non-numeric to NaN\n",
    "    data_cleaned = data_cleaned.fillna(data_cleaned.mean(numeric_only=True))  # Fill NaN values with column means\n",
    "    context.log.info(\"Data cleaned successfully for dataset 1\")\n",
    "    return data_cleaned\n",
    "\n",
    "@op\n",
    "def wave_buoy_sligo_clean_data(context, data):\n",
    "    columns_to_drop = [\n",
    "        'longitude', 'latitude', 'station_id'\n",
    "    ]\n",
    "    data_cleaned = data.drop(columns=columns_to_drop, errors='ignore')\n",
    "    data_cleaned = data_cleaned.apply(pd.to_numeric, errors='coerce')  # Convert non-numeric to NaN\n",
    "    data_cleaned = data_cleaned.fillna(data_cleaned.mean(numeric_only=True))  # Fill NaN values with column means\n",
    "    context.log.info(\"Data cleaned successfully for dataset 2\")\n",
    "    return data_cleaned\n",
    "\n",
    "@op\n",
    "def galway_bay_test_site_transform_data(context, data):\n",
    "    target_column = 'Power_Output'\n",
    "    required_columns = [\n",
    "        \"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X7\", \"X8\", \"X9\", \"X10\", \"X11\", \"X12\", \"X13\", \"X14\", \"X15\", \"X16\",\n",
    "        \"Y1\", \"Y2\", \"Y3\", \"Y4\", \"Y5\", \"Y6\", \"Y7\", \"Y8\", \"Y9\", \"Y10\", \"Y11\", \"Y12\", \"Y13\", \"Y14\", \"Y15\", \"Y16\",\n",
    "        \"Power_Output\", \"wave_height\"\n",
    "    ]\n",
    "    \n",
    "    # Sample the data\n",
    "    data_sample = data.sample(frac=0.1, random_state=42)\n",
    "    \n",
    "    # Creating the wave_height column\n",
    "    p_columns = [f'P{i}' for i in range(1, 17)]\n",
    "    data_sample['wave_height'] = data_sample[p_columns].sum(axis=1) + 1\n",
    "    \n",
    "    # Combine original data with wave_height\n",
    "    data_transformed = data_sample[required_columns]\n",
    "\n",
    "    context.log.info(\"Data transformed successfully for dataset 1\")\n",
    "    return data_transformed\n",
    "\n",
    "@op\n",
    "def wave_buoy_sligo_transform_data(context, data):\n",
    "    target_column = 'SignificantWaveHeight'\n",
    "    \n",
    "    # Sample the data\n",
    "    data_sample = data.sample(frac=0.1, random_state=42)\n",
    "    \n",
    "    # Convert 'time' column to datetime\n",
    "    if 'time' in data_sample.columns:\n",
    "        data_sample['time'] = pd.to_datetime(data_sample['time'], errors='coerce')\n",
    "    else:\n",
    "        context.log.warning(\"'time' column not found in the dataset.\")\n",
    "    \n",
    "    # Drop columns with high percentage of NaN values\n",
    "    nan_threshold = 0.5\n",
    "    data_sample = data_sample.dropna(thresh=int(nan_threshold * len(data_sample)), axis=1)\n",
    "    \n",
    "    context.log.info(f\"Columns after dropping those with more than {nan_threshold * 100}% NaNs: {data_sample.columns.tolist()}\")\n",
    "\n",
    "    # Ensure there are still rows left after dropping NaNs\n",
    "    if data_sample.empty:\n",
    "        context.log.error(\"Data sample is empty after dropping columns with high NaN values.\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame to handle gracefully\n",
    "\n",
    "    # Creating the WavePeriod column\n",
    "    data_sample['WavePeriod'] = data_sample['PeakPeriod'] + data_sample['UpcrossPeriod']\n",
    "    data_sample = data_sample.drop(columns=['PeakPeriod', 'UpcrossPeriod'])\n",
    "\n",
    "    # Combine original data with WavePeriod\n",
    "    data_transformed = data_sample\n",
    "\n",
    "    # Re-add the 'time' column if it exists\n",
    "    if 'time' in data_sample.columns:\n",
    "        data_transformed['time'] = data_sample['time'].iloc[data_sample.shape[0] - data_transformed.shape[0]:].values\n",
    "    else:\n",
    "        # Create a new 'time' column with random timestamps from 2006 to 2024\n",
    "        timestamps = pd.date_range(start='2006-01-01', end='2024-12-31', periods=len(data_transformed))\n",
    "        np.random.shuffle(timestamps.values)\n",
    "        data_transformed['time'] = timestamps\n",
    "\n",
    "    context.log.info(\"Data transformed successfully for dataset 2\")\n",
    "    return data_transformed\n",
    "\n",
    "@op\n",
    "def join_datasets(context, test_site_data, buoy_data):\n",
    "    # Join the datasets by row-wise concatenation\n",
    "    data_combined = pd.concat([test_site_data.reset_index(drop=True), buoy_data.reset_index(drop=True)], axis=1)\n",
    "    context.log.info(\"Datasets joined successfully\")\n",
    "    return data_combined\n",
    "\n",
    "@op\n",
    "def load_data(context, data_combined):\n",
    "    # Create a connection to the database\n",
    "    engine = create_engine(connection_string)\n",
    "    \n",
    "    # Insert data in smaller batches\n",
    "    for i in range(0, len(data_combined), batch_size):\n",
    "        batch = data_combined.iloc[i:i+batch_size]\n",
    "        batch.to_sql('test', engine, index=False, if_exists='append' if i > 0 else 'replace')\n",
    "        context.log.info(f\"Batch {i//batch_size + 1} loaded into PostgreSQL successfully\")\n",
    "        \n",
    "@job\n",
    "def etl_job():\n",
    "    test_site_data = galway_bay_test_site_extract_data()\n",
    "    cleaned_test_site_data = galway_bay_test_site_clean_data(test_site_data)\n",
    "    transformed_test_site_data = galway_bay_test_site_transform_data(cleaned_test_site_data)\n",
    "    \n",
    "    buoy_data = wave_buoy_sligo_extract_data()\n",
    "    cleaned_buoy_data = wave_buoy_sligo_clean_data(buoy_data)\n",
    "    transformed_buoy_data = wave_buoy_sligo_transform_data(cleaned_buoy_data)\n",
    "    \n",
    "    data_combined = join_datasets(transformed_test_site_data, transformed_buoy_data)\n",
    "    load_data(data_combined)\n",
    "\"\"\"\n",
    "\n",
    "with open(\"my_etl_pipeline.py\", \"w\") as file:\n",
    "    file.write(etl_pipeline_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6598dc49-c357-401a-a5c3-3da147d0f452",
   "metadata": {},
   "outputs": [],
   "source": [
    "repository_content = \"\"\"\n",
    "import sys\n",
    "import os\n",
    "import importlib.util\n",
    "\n",
    "# Add the current working directory to the Python path\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "from dagster import repository\n",
    "\n",
    "# Dynamic import of my_etl_pipeline module\n",
    "module_name = 'my_etl_pipeline'\n",
    "module_path = os.path.join(os.getcwd(), 'my_etl_pipeline.py')\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(module_name, module_path)\n",
    "my_etl_pipeline = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(my_etl_pipeline)\n",
    "\n",
    "@repository\n",
    "def my_repository():\n",
    "    return [my_etl_pipeline.etl_job]\n",
    "\"\"\"\n",
    "\n",
    "with open(\"repository.py\", \"w\") as file:\n",
    "    file.write(repository_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c4ad9eb-5df0-4cc3-9534-50abff7551bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: C:\\Users\\sabar\\Project Wave Energy optimization\n",
      "\n",
      "Directory Contents:\n",
      ".ipynb_checkpoints\n",
      "ETL_Wave_power_optimization.ipynb\n",
      "Galway bay test site.csv\n",
      "hyperparameter_tuning\n",
      "kt_dir\n",
      "my_dir\n",
      "my_etl_pipeline.py\n",
      "repository.py\n",
      "test.ipynb\n",
      "tmp2wzj4s5k\n",
      "tmp3g9iwyfb\n",
      "tmp96pwxfgl\n",
      "tmpjphgzp8z\n",
      "tmpntbxgo7e\n",
      "tmpo2feeq72\n",
      "tmpoj6_83j5\n",
      "Untitled.ipynb\n",
      "Wave buoy_Sligo.csv\n",
      "Wave energy generation optimization.ipynb\n",
      "Wave energy Optimization_ETL.ipynb\n",
      "Wave forecast_Sligo area.ipynb\n",
      "__pycache__\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Print the current working directory\n",
    "print(f\"Current Working Directory: {os.getcwd()}\")\n",
    "\n",
    "# List all files and directories in the current working directory\n",
    "print(\"\\nDirectory Contents:\")\n",
    "for item in os.listdir(os.getcwd()):\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe87faa-8014-4401-9d5a-b1a632cf45f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ee9646-cc16-48bb-a1ae-aa90b25ab675",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
